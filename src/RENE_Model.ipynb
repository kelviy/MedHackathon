{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7yQy_vx5vaa",
        "outputId": "19ba03a9-e8ad-4738-c15f-6aa7f38515ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "#!pip install openai-whisper git+https://github.com/sooftware/conformer.git PyYAML gdown gradio -q\n",
        "import torch\n",
        "from cfg_parse import models_folder_path, cfg, class_file, data_base_path\n",
        "# Check that we have a GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3K622F3CMv7",
        "outputId": "d187d58b-3eb0-4cb7-824c-efc75c779a9c"
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "# # Create cfg.yaml with model parameters (adapted from the official repo)\n",
        "# cat > ../models/RENE/cfg.yaml << 'CFG'\n",
        "# # Data and model config\n",
        "# device: 'cuda:0'        # computation device\n",
        "# sampling_rate: 8000     # audio sampling rate\n",
        "# win_len: 256            # STFT window length (25ms)\n",
        "# hop: 80                 # STFT hop length (10ms)\n",
        "# lowfreq: 50.0           # mel filterbank low freq cutoff\n",
        "# highfreq: 2500.0        # mel filterbank high freq cutoff\n",
        "# max_record_time: 16     # max duration of each recording (s)\n",
        "# max_event_time: 3       # max duration of each respiratory event (s)\n",
        "# # Model hyperparameters\n",
        "# whisper_seq: 1500\n",
        "# whiper_dim: 384\n",
        "# encoder_dim: 256\n",
        "# num_encoder_layers: 16\n",
        "# num_attention_heads: 4\n",
        "# rnn_hid_dim: 512\n",
        "# rnn_layers: 2\n",
        "# bidirect: true\n",
        "# n_fc_layers: 2\n",
        "# fc_layer_dim: 1024\n",
        "# output_dim: 15\n",
        "# input_dropout: 0.1\n",
        "# feed_forward_dropout: 0.1\n",
        "# attention_dropout: 0.1\n",
        "# conv_dropout: 0.1\n",
        "# rtb_data_channels: 1\n",
        "# CFG\n",
        "\n",
        "# %%bash\n",
        "# # Create class-id.txt mapping 15 classes (Name|ID)\n",
        "# cat > ../models/RENE/class-id.txt << 'CLASSIDS'\n",
        "# Healthy|0\n",
        "# Bronchiectasis|1\n",
        "# Bronchiolitis|2\n",
        "# COPD|3\n",
        "# Asthma|4\n",
        "# LRTI|5\n",
        "# Pneumonia|6\n",
        "# URTI|7\n",
        "# Bronchitis|8\n",
        "# Lung Fibrosis|9\n",
        "# Asthma & Lung Fibrosis|10\n",
        "# Heart Failure & Lung Fibrosis|11\n",
        "# Heart Failure|12\n",
        "# Heart Failure & COPD|13\n",
        "# Pleural Effusion|14\n",
        "# CLASSIDS\n",
        "\n",
        "# %%bash\n",
        "# # Create cfg_parse.py to load the YAML config\n",
        "# cat > ../models/RENE/cfg_parse.py << 'PYCODE'\n",
        "# import yaml\n",
        "# cfg = yaml.safe_load(open('cfg.yaml'))\n",
        "# PYCODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9T9y4bACN41",
        "outputId": "9f8a0d11-5b37-4117-a3a5-7d6c84f79cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model instantiated with 15 output classes.\n"
          ]
        }
      ],
      "source": [
        "# Import the config and define model architecture classes\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from conformer import Conformer\n",
        "\n",
        "# Depthwise Separable Conv2D layer used in ReneTrialBlock\n",
        "class DSConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        super(DSConv2d, self).__init__()\n",
        "        self.depth_conv = nn.Conv2d(\n",
        "            in_channels=in_channels, out_channels=in_channels,\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            padding=(kernel_size // 2, kernel_size // 2), groups=in_channels\n",
        "        )\n",
        "        self.pointwise_conv = nn.Conv2d(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=(1, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = self.depth_conv(x)\n",
        "        out = self.pointwise_conv(out)\n",
        "        return out\n",
        "\n",
        "# ReneTrialBlock: the final convolutional block that produces class logits\n",
        "class ReneTrialBlock(nn.Module):\n",
        "    def __init__(self, cfg, in_channels):\n",
        "        super(ReneTrialBlock, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        # Left convolution flow\n",
        "        self.left_flow = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(1,1)),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            DSConv2d(in_channels, in_channels, kernel_size=3),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(5,5), padding=(5//2, 5//2))\n",
        "        )\n",
        "        # Right convolution flow (mirror of left_flow with reversed conv order)\n",
        "        self.right_flow = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(5,5), padding=(5//2, 5//2)),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            DSConv2d(in_channels, in_channels, kernel_size=3),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(1,1))\n",
        "        )\n",
        "        # Final linear layer: maps concatenated features to output classes\n",
        "        self.layer = nn.Linear(cfg['rnn_hid_dim'] * 2, cfg['output_dim'])\n",
        "    def forward(self, input_data):\n",
        "        # input_data: [batch, channels*feature_map] as a flat vector\n",
        "        # Reshape to 2D feature maps (assume square)\n",
        "        feature_size = int(math.sqrt(cfg['rnn_hid_dim'] * 2))\n",
        "        x = input_data.reshape(input_data.size(0), cfg['rtb_data_channels'], feature_size, feature_size)\n",
        "        # Convolution flows and residual\n",
        "        out = self.left_flow(x) + self.right_flow(x) + x\n",
        "        # Flatten and linear layer to class logits\n",
        "        out = out.view(input_data.size(0), -1)\n",
        "        return self.layer(out)\n",
        "\n",
        "# Main RENE Model class\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(Model, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        # Conformer encoder (from the installed library)\n",
        "        self.conformer = Conformer(\n",
        "            num_classes=cfg['rnn_hid_dim'],      # output feature dim = rnn hidden dim\n",
        "            input_dim=cfg['whiper_dim'],         # Whisper encoder feature dimension\n",
        "            encoder_dim=cfg['encoder_dim'],\n",
        "            num_encoder_layers=cfg['num_encoder_layers'],\n",
        "            num_attention_heads=cfg['num_attention_heads'],\n",
        "            input_dropout_p=cfg['input_dropout'],\n",
        "            feed_forward_dropout_p=cfg['feed_forward_dropout'],\n",
        "            attention_dropout_p=cfg['attention_dropout'],\n",
        "            conv_dropout_p=cfg['conv_dropout']\n",
        "        )\n",
        "        # Bidirectional GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=cfg['rnn_hid_dim'], hidden_size=cfg['rnn_hid_dim'],\n",
        "            num_layers=cfg['rnn_layers'], bidirectional=cfg['bidirect']\n",
        "        )\n",
        "        # ReneTrialBlock for final classification\n",
        "        self.rene = ReneTrialBlock(cfg, in_channels=cfg['rtb_data_channels'])\n",
        "    def forward(self, x, input_lengths):\n",
        "        # x: [batch, time_frames, whisper_dim], input_lengths: length of each sequence\n",
        "        encoder_out, output_lengths = self.conformer(x, input_lengths)  # [batch, T, rnn_hid_dim]\n",
        "        # Transpose to shape [T, batch, features] for GRU\n",
        "        encoder_out = encoder_out.permute(1, 0, 2)\n",
        "        rnn_out, _ = self.gru(encoder_out)         # rnn_out: [T, batch, 2*rnn_hid_dim] (bi-GRU)\n",
        "        last_timestep = rnn_out[-1]                # take the last time-step output of GRU for each batch\n",
        "        logits = self.rene(last_timestep)    # [batch, output_dim] = class scores\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model (we will load pretrained weights next)\n",
        "model = Model(cfg)\n",
        "print(\"Model instantiated with %d output classes.\" % model.cfg['output_dim'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeRMZ89ZCSMS",
        "outputId": "c650127e-a367-442e-af3a-a4ea128ecc56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model checkpoint already downloaded.\n"
          ]
        }
      ],
      "source": [
        "# Download the RENE(S) pretrained checkpoint from Google Drive\n",
        "import os\n",
        "model_path = models_folder_path / \"Rene.pth\"\n",
        "if not os.path.exists(model_path):\n",
        "    # Using gdown with the shared file ID\n",
        "    !gdown --id 1NcGPIURY4mWtRr_KkwHAodssOexN-PbC -O {model_path}\n",
        "else:\n",
        "    print(\"Model checkpoint already downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyXaV0Y7CUkq",
        "outputId": "a9eb6ce8-ab50-41a4-96a4-1b89502e9f45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained RENE model loaded.\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained weights into the model\n",
        "\n",
        "checkpoint = torch.load(model_path, map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model_state_dict'], strict=False) \n",
        "\n",
        "# load fine tuned model\n",
        "# checkpoint = torch.load(\"../models/RENE/fine_tuned_rene_model.pt\", map_location='cpu')\n",
        "# model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "model.to(device).eval()\n",
        "print(\"Pretrained RENE model loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Model with one sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uaEoapB3CVos"
      },
      "outputs": [],
      "source": [
        "# Download a sample lung sound WAV (from SPRSound open dataset)\n",
        "sample_url = \"https://raw.githubusercontent.com/SJTU-YONGFU-RESEARCH-GRP/SPRSound/main/example/65097128_5.6_1_p1_2242.wav\"\n",
        "sample_path = data_base_path / \"sample.wav\"\n",
        "!wget -q -O $sample_path $sample_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHt0vAUoCXCS",
        "outputId": "14aa3cb0-6a94-4856-9e68-0a2a2527a537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top predicted class: **Healthy** (14.9% confidence)\n",
            "\n",
            "Class probabilities:\n",
            "  Healthy                  : 14.90%\n",
            "  URTI                     : 14.63%\n",
            "  Asthma & Lung Fibrosis   : 10.62%\n",
            "  Pneumonia                : 10.19%\n",
            "  Heart Failure            : 6.69%\n",
            "  Heart Failure & Lung Fibrosis: 5.62%\n",
            "  Bronchiolitis            : 5.52%\n",
            "  Pleural Effusion         : 5.17%\n",
            "  Bronchitis               : 5.09%\n",
            "  Lung Fibrosis            : 4.88%\n",
            "  LRTI                     : 3.94%\n",
            "  Asthma                   : 3.70%\n",
            "  Heart Failure & COPD     : 3.38%\n",
            "  COPD                     : 2.96%\n",
            "  Bronchiectasis           : 2.73%\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "# Load Whisper tiny model for feature extraction\n",
        "whisper_model = whisper.load_model(\"tiny\").to(device)\n",
        "whisper_model.eval()\n",
        "\n",
        "# Load and preprocess the audio\n",
        "audio = whisper.load_audio(data_base_path / \"sample.wav\")  # returns NumPy array in float32\n",
        "# Whisper expects 16 kHz audio and pads/clips to 30 sec. We'll pad/trim to 16 sec (target of RENE)\n",
        "MAX_SEC = 30\n",
        "audio = whisper.pad_or_trim(audio, length=MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "\n",
        "# Use Whisper encoder to get audio features\n",
        "with torch.no_grad():\n",
        "    encoder_out = whisper_model.encoder(mel.unsqueeze(0).to(device))  # shape [1, n_frames, 384]\n",
        "# Determine actual length in frames (to inform Conformer)\n",
        "n_frames_total = encoder_out.shape[1]  # typically 1500 for 15s of audio after Whisper padding\n",
        "# Estimate the number of frames corresponding to real (non-padded) audio content\n",
        "orig_len_samples = min(len(audio), MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "orig_frames = math.floor(orig_len_samples / 160)  # 160-sample hop = 10ms frame step\n",
        "input_length = torch.LongTensor([orig_frames // 2])  # //2 because Whisper encoder downsamples by 2x in time\n",
        "\n",
        "# Run the RENE model to get class logits\n",
        "encoder_out = encoder_out.to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(encoder_out, input_length.to(device))\n",
        "    probs = torch.softmax(logits, dim=1)[0]  # probabilities for each of the 15 classes\n",
        "\n",
        "# Load class names and print results\n",
        "classes = class_file\n",
        "top_idx = int(torch.argmax(probs))\n",
        "top_class = classes[top_idx]\n",
        "top_conf = probs[top_idx].item()\n",
        "\n",
        "print(f\"Top predicted class: **{top_class}** ({top_conf*100:.1f}% confidence)\")\n",
        "print(\"\\nClass probabilities:\")\n",
        "ranked = sorted(zip(classes, probs.cpu().numpy()), key=lambda x: x[1], reverse=True)\n",
        "for cls, p in ranked:\n",
        "    print(f\"  {cls:25s}: {p*100:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "ag5X2FpKCY6U",
        "outputId": "3943c6b5-a1e3-4766-ffed-7885d84a7e74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mistycloud/Projects/MedHackathon/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://41a7713c9f7044c3b4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://41a7713c9f7044c3b4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define the prediction function for Gradio\n",
        "def classify_respiratory_sound(audio_file):\n",
        "    # Load audio from the uploaded file\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    audio = whisper.pad_or_trim(audio, length=MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "    with torch.no_grad():\n",
        "        enc_out = whisper_model.encoder(mel.unsqueeze(0).to(device))\n",
        "    # Calculate original length in frames for masking\n",
        "    orig_len = min(len(audio), MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "    orig_frames = math.floor(orig_len / 160)\n",
        "    inp_len = torch.LongTensor([orig_frames // 2])\n",
        "    with torch.no_grad():\n",
        "        logits = model(enc_out.to(device), inp_len.to(device))\n",
        "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "    # Prepare outputs\n",
        "    top_idx = int(probs.argmax())\n",
        "    top_label = classes[top_idx]\n",
        "    # Build dict of class confidences\n",
        "    confidences = {cls: float(probs[i]) for i, cls in enumerate(classes)}\n",
        "    return top_label, confidences\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=classify_respiratory_sound,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload Lung Sound (.wav)\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Top Predicted Disease\"),\n",
        "        gr.Label(num_top_classes=15, label=\"All Class Probabilities\")\n",
        "    ],\n",
        "    title=\"RENE Respiratory Disease Classifier\",\n",
        "    description=\"Upload a lung sound recording (.wav) to get the predicted respiratory condition and confidence scores for all 15 classes.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app (in Colab, this will display an inline interface or a shareable link)\n",
        "interface.launch(debug=False, share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lqd6K5CFFzD"
      },
      "source": [
        "## Testing accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0S6lSrQFFgk",
        "outputId": "b52398c7-3885-483c-8944-203f2eee3751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/vbookshelf/respiratory-sound-database?dataset_version_number=2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3.69G/3.69G [20:48<00:00, 3.17MB/s] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moved to: /Users/mistycloud/Projects/MedHackathon/data/respiratory-sound-database\n",
            "Path to dataset files: /Users/mistycloud/Projects/MedHackathon/data/respiratory-sound-database\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Download latest version\n",
        "custom_path = data_base_path / \"respiratory-sound-database\"\n",
        "path = kagglehub.dataset_download(\"vbookshelf/respiratory-sound-database\")\n",
        "shutil.move(path, custom_path)\n",
        "\n",
        "path = custom_path\n",
        "print(\"Moved to:\", custom_path)\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxzXL7x1FJLw",
        "outputId": "4c4d6adf-a635-48ad-f586-003065f8d7bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 920 .wav files.\n",
            "Labeled file saved to: /Users/mistycloud/Projects/MedHackathon/data/labeled_wav_files.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration: Update these paths as needed.\n",
        "# -----------------------------\n",
        "wav_folder = data_base_path / \"respiratory-sound-database\" / \"Respiratory_Sound_Database\" / \"Respiratory_Sound_Database\" / \"audio_and_txt_files\"               # Folder containing all the .wav files\n",
        "diagnosis_csv = data_base_path / \"respiratory-sound-database\" / \"Respiratory_Sound_Database\" / \"Respiratory_Sound_Database\" / \"patient_diagnosis.csv\"  # CSV file with patient diagnoses\n",
        "label_csv = data_base_path / \"labeled_wav_files.csv\"             # Output CSV file\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Load the patient diagnoses CSV.\n",
        "# The CSV provided is in the format:\n",
        "# 101,URTI\n",
        "# 102,Healthy\n",
        "# ...\n",
        "# If the file does not have a header, we specify header=None.\n",
        "# -----------------------------\n",
        "diag_df = pd.read_csv(diagnosis_csv, header=None, names=[\"patient_id\", \"diagnosis\"])\n",
        "\n",
        "# Convert patient IDs to strings (to match with the extracted file name parts)\n",
        "diag_df[\"patient_id\"] = diag_df[\"patient_id\"].astype(str)\n",
        "\n",
        "# Create a mapping from patient_id to diagnosis.\n",
        "diagnosis_map = diag_df.set_index(\"patient_id\")[\"diagnosis\"].to_dict()\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: List all .wav files in the specified directory.\n",
        "# -----------------------------\n",
        "wav_files = [f for f in os.listdir(wav_folder) if f.lower().endswith(\".wav\")]\n",
        "print(f\"Found {len(wav_files)} .wav files.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: For each .wav file, extract the patient ID and look up the diagnosis.\n",
        "# Here we assume the file names start with the patient ID followed by an underscore.\n",
        "# For example: '101_1b1_Al_sc_Meditron.wav' -> Patient ID: '101'\n",
        "# -----------------------------\n",
        "labeled_records = []\n",
        "for file_name in wav_files:\n",
        "    # Extract patient ID by splitting at the underscore.\n",
        "    patient_id = file_name.split(\"_\")[0]\n",
        "    # Retrieve the diagnosis from the mapping.\n",
        "    diagnosis = diagnosis_map.get(patient_id, \"unknown\")\n",
        "    labeled_records.append({\n",
        "        \"wav_file\": file_name,\n",
        "        \"patient_id\": patient_id,\n",
        "        \"diagnosis\": diagnosis\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Save the results into an output CSV.\n",
        "# -----------------------------\n",
        "labeled_df = pd.DataFrame(labeled_records)\n",
        "labeled_df.to_csv(label_csv, index=False)\n",
        "print(f\"Labeled file saved to: {label_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test utilising total dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kbgmbs8F8Rx",
        "outputId": "be622ca0-1c75-4565-f5c3-252d2db6846c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 920 label entries.\n",
            "[1] File: 122_2b1_Tc_mc_LittC2SE.wav | True: Pneumonia | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[2] File: 113_1b1_Lr_sc_Litt3200.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[3] File: 178_1b3_Pl_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[4] File: 220_1b1_Tc_mc_LittC2SE.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[5] File: 176_1b3_Tc_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[6] File: 154_2b4_Ll_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[7] File: 117_1b3_Tc_mc_LittC2SE.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[8] File: 218_1b1_Lr_sc_Meditron.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[9] File: 185_1b1_Ll_sc_Litt3200.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[10] File: 130_2b4_Pl_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[11] File: 156_8b3_Pl_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[12] File: 172_1b5_Al_mc_AKGC417L.wav | True: COPD | Top1: Healthy | Top3: ['Healthy', 'URTI', 'Asthma & Lung Fibrosis']\n",
            "[13] File: 200_3p4_Pr_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[14] File: 193_1b2_Tc_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Pneumonia', 'Healthy']\n",
            "[15] File: 201_1b2_Ar_sc_Meditron.wav | True: Bronchiectasis | Top1: Healthy | Top3: ['Healthy', 'URTI', 'Asthma & Lung Fibrosis']\n",
            "[16] File: 112_1b1_Ar_sc_Meditron.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[17] File: 133_2p4_Tc_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[18] File: 122_2b3_Ar_mc_LittC2SE.wav | True: Pneumonia | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[19] File: 162_2b3_Lr_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[20] File: 134_2b1_Ar_mc_LittC2SE.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[21] File: 157_1b1_Pr_sc_Meditron.wav | True: COPD | Top1: Healthy | Top3: ['Healthy', 'URTI', 'Asthma & Lung Fibrosis']\n",
            "[22] File: 164_1b1_Ll_sc_Meditron.wav | True: URTI | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[23] File: 160_1b3_Ar_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[24] File: 174_2p3_Pr_mc_AKGC417L.wav | True: COPD | Top1: Healthy | Top3: ['Healthy', 'URTI', 'Asthma & Lung Fibrosis']\n",
            "[25] File: 158_2p3_Tc_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[26] File: 158_1p4_Lr_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[27] File: 196_1b1_Pr_sc_Meditron.wav | True: Bronchiectasis | Top1: Healthy | Top3: ['Healthy', 'URTI', 'Pneumonia']\n",
            "[28] File: 145_2b2_Al_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[29] File: 172_1b3_Pl_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[30] File: 198_1b5_Ll_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[31] File: 130_2b2_Al_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[32] File: 186_3b3_Pr_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[33] File: 133_3p2_Pl_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[34] File: 203_1p3_Pr_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[35] File: 221_2b3_Al_mc_LittC2SE.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[36] File: 207_3b2_Ar_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[37] File: 130_1p3_Al_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[38] File: 202_1b1_Ar_sc_Meditron.wav | True: Healthy | Top1: Healthy | Top3: ['Healthy', 'URTI', 'Asthma & Lung Fibrosis']\n",
            "[39] File: 170_1b3_Lr_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[40] File: 203_2p3_Al_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[41] File: 139_1b1_Pr_sc_Litt3200.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[42] File: 109_1b1_Ar_sc_Litt3200.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[43] File: 223_1b1_Lr_sc_Meditron.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[44] File: 130_3b3_Ll_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[45] File: 138_2p2_Tc_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[46] File: 200_2p3_Lr_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[47] File: 178_1b6_Lr_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[48] File: 107_2b3_Ar_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[49] File: 205_1b3_Ar_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[50] File: 101_1b1_Al_sc_Meditron.wav | True: URTI | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[51] File: 151_2p2_Ar_mc_AKGC417L.wav | True: COPD | Top1: Healthy | Top3: ['Healthy', 'URTI', 'Asthma & Lung Fibrosis']\n",
            "[52] File: 130_2p5_Ar_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Pneumonia']\n",
            "[53] File: 175_1b1_Ll_sc_Litt3200.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Run the Whisper encoder to get features.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     encoder_out = \u001b[43mwhisper_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: [1, n_frames, 384]\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Determine the number of frames corresponding to the original audio.\u001b[39;00m\n\u001b[32m     67\u001b[39m orig_len_samples = \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(audio), MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/MedHackathon/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/MedHackathon/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/MedHackathon/.venv/lib/python3.11/site-packages/whisper/model.py:201\u001b[39m, in \u001b[36mAudioEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    198\u001b[39m x = (x + \u001b[38;5;28mself\u001b[39m.positional_embedding).to(x.dtype)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_post(x)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/MedHackathon/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/MedHackathon/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/MedHackathon/.venv/lib/python3.11/site-packages/whisper/model.py:170\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cross_attn:\n\u001b[32m    169\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.cross_attn(\u001b[38;5;28mself\u001b[39m.cross_attn_ln(x), xa, kv_cache=kv_cache)[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m(\u001b[38;5;28mself\u001b[39m.mlp_ln(x))\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/MedHackathon/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1915\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1912\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1913\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1914\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1916\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1917\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Tests accuracy of total dataset\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "classes = class_file         # Text file containing class names (each line: \"ClassName| ...\")\n",
        "MAX_SEC = 30  # Maximum duration (in seconds) for padding/trimming\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load Whisper model for feature extraction\n",
        "# -----------------------------\n",
        "whisper_model = whisper.load_model(\"tiny\").to(device)\n",
        "whisper_model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Load your pre-trained RENE classification model.\n",
        "# Make sure to load your model and move it to 'device' and set it to eval mode.\n",
        "# Example (update with your actual code):\n",
        "# model = torch.load(\"path/to/your/rene_model.pt\", map_location=device)\n",
        "# model.eval()\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Read the CSV file containing true labels.\n",
        "# Expected CSV format: wav_file,patient_id,diagnosis\n",
        "# -----------------------------\n",
        "df_labels = pd.read_csv(label_csv)\n",
        "print(\"Loaded {} label entries.\".format(len(df_labels)))\n",
        "\n",
        "# -----------------------------\n",
        "# Loop through each file, run through the pipeline, and collect predictions.\n",
        "# For each file, check if the true label is present among the top 3 predictions.\n",
        "# -----------------------------\n",
        "correct = 0\n",
        "total = 0\n",
        "results = []  # To store (file, true_label, predicted_top1, predicted_top3)\n",
        "\n",
        "for idx, row in df_labels.iterrows():\n",
        "    wav_file = row['wav_file']\n",
        "    true_label = row['diagnosis'].strip()  # True diagnosis label (e.g., \"COPD\", \"Healthy\", etc.)\n",
        "    wav_path = os.path.join(wav_folder, wav_file)\n",
        "\n",
        "    # Load and preprocess the audio using Whisper utilities.\n",
        "    audio = whisper.load_audio(wav_path)  # Loads audio as NumPy float32 array\n",
        "    audio = whisper.pad_or_trim(audio, length=MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "\n",
        "    # Get the log-mel spectrogram of the audio.\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "\n",
        "    # Run the Whisper encoder to get features.\n",
        "    with torch.no_grad():\n",
        "        encoder_out = whisper_model.encoder(mel.unsqueeze(0).to(device))  # shape: [1, n_frames, 384]\n",
        "\n",
        "    # Determine the number of frames corresponding to the original audio.\n",
        "    orig_len_samples = min(len(audio), MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "    orig_frames = math.floor(orig_len_samples / 160)  # 160-sample hop (10ms per hop)\n",
        "    input_length = torch.LongTensor([orig_frames // 2])  # Encoder downsamples time by 2×\n",
        "\n",
        "    # Run the RENE classification model.\n",
        "    encoder_out = encoder_out.to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(encoder_out, input_length.to(device))\n",
        "        probs = torch.softmax(logits, dim=1)[0]  # probabilities for each class\n",
        "\n",
        "    # Get the top 3 predicted indices.\n",
        "    topk = torch.topk(probs, k=3)\n",
        "    top_indices = topk.indices.cpu().numpy()\n",
        "    # Top predicted class (highest probability)\n",
        "    top1_label = classes[int(torch.argmax(probs))]\n",
        "    # Top 3 predicted class names\n",
        "    top3_labels = [classes[i] for i in top_indices]\n",
        "\n",
        "    # Count as correct if the true label is among the top 3 predictions.\n",
        "    if true_label.lower() in [label.lower() for label in top3_labels]:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "    results.append((wav_file, true_label, top1_label, top3_labels))\n",
        "    print(f\"[{total}] File: {wav_file} | True: {true_label} | Top1: {top1_label} | Top3: {top3_labels}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Compute and print overall top-3 accuracy.\n",
        "# -----------------------------\n",
        "accuracy = (correct / total) * 100 if total > 0 else 0\n",
        "print(f\"\\nOverall Top-3 Accuracy: {accuracy:.2f}% ({correct} / {total} correct)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine Tuning Model with data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "IE5Sigy5KP9Q",
        "outputId": "6a188442-c592-4f6f-fcb7-8530db805870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Classes: ['Healthy', 'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Asthma', 'LRTI', 'Pneumonia', 'URTI', 'Bronchitis', 'Lung Fibrosis', 'Asthma & Lung Fibrosis', 'Heart Failure & Lung Fibrosis', 'Heart Failure', 'Heart Failure & COPD', 'Pleural Effusion']\n",
            "Class to index mapping: {'Asthma': 0, 'Asthma & Lung Fibrosis': 1, 'Bronchiectasis': 2, 'Bronchiolitis': 3, 'Bronchitis': 4, 'COPD': 5, 'Healthy': 6, 'Heart Failure': 7, 'Heart Failure & COPD': 8, 'Heart Failure & Lung Fibrosis': 9, 'LRTI': 10, 'Lung Fibrosis': 11, 'Pleural Effusion': 12, 'Pneumonia': 13, 'URTI': 14}\n",
            "Training samples: 736 Validation samples: 184\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-639d7d5435b8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-107-639d7d5435b8>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;31m# Move data to device.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# shape: [batch, n_frames, feature_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-107-639d7d5435b8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# Load and preprocess audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Returns a NumPy float32 array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Pad or trim audio to fixed length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_or_trim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_sec\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# fmt: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load audio: {e.stderr.decode()}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "classes = class_file\n",
        "MAX_SEC = 30  # Maximum duration in seconds for padding/trimming\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Load Whisper model for feature extraction\n",
        "# -----------------------------\n",
        "whisper_model = whisper.load_model(\"tiny\").to(device)\n",
        "whisper_model.eval()\n",
        "# Freeze Whisper encoder parameters so that they are not updated during fine-tuning\n",
        "for param in whisper_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# -----------------------------\n",
        "# Load your pre-trained RENE classification model.\n",
        "# Replace the following placeholder with your actual model loading code.\n",
        "# For example:\n",
        "# model = torch.load(\"path/to/your/rene_model.pt\", map_location=device)\n",
        "# model.train()  # Set to training mode for fine-tuning.\n",
        "# -----------------------------\n",
        "model.to(device)\n",
        "model.train()  # Set model to training mode\n",
        "\n",
        "# -----------------------------\n",
        "# Load class names from class-id.txt.\n",
        "# Each line in the file is split at the '|' character and the first token is used.\n",
        "# -----------------------------\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "# Create a mapping from class name to index (we assume the CSV 'diagnosis' field exactly matches one of these class names).\n",
        "class_to_idx = {cls: i for i, cls in enumerate(classes)}\n",
        "num_classes = len(class_to_idx)\n",
        "print(\"Class to index mapping:\", class_to_idx)\n",
        "\n",
        "# -----------------------------\n",
        "# Define a custom dataset.\n",
        "# For each entry, load the .wav file, extract features using Whisper encoder,\n",
        "# and return (encoder_output, input_length, label) for training.\n",
        "# -----------------------------\n",
        "class RespiratoryDataset(Dataset):\n",
        "    def __init__(self, csv_file, wav_folder, max_sec=30, whisper_model=whisper_model, class_to_idx=class_to_idx):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.wav_folder = wav_folder\n",
        "        self.max_sec = max_sec\n",
        "        self.whisper_model = whisper_model\n",
        "        self.class_to_idx = class_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        wav_file = row['wav_file']\n",
        "        true_label = row['diagnosis'].strip()\n",
        "        label_idx = self.class_to_idx[true_label]\n",
        "        wav_path = os.path.join(self.wav_folder, wav_file)\n",
        "\n",
        "        # Load and preprocess audio\n",
        "        audio = whisper.load_audio(wav_path)  # Returns a NumPy float32 array.\n",
        "        # Pad or trim audio to fixed length\n",
        "        audio = whisper.pad_or_trim(audio, length=self.max_sec * whisper.audio.SAMPLE_RATE)\n",
        "        # Compute log-mel spectrogram\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "\n",
        "        # Pass through the Whisper encoder (without gradient updates)\n",
        "        with torch.no_grad():\n",
        "            encoder_out = self.whisper_model.encoder(mel.unsqueeze(0).to(device))  # shape: [1, n_frames, 384]\n",
        "\n",
        "        # Determine the number of frames corresponding to the original (non-padded) audio.\n",
        "        orig_len_samples = min(len(audio), self.max_sec * whisper.audio.SAMPLE_RATE)\n",
        "        orig_frames = math.floor(orig_len_samples / 160)  # Whisper uses a hop of 160 samples (~10ms)\n",
        "        # The encoder downsamples time by a factor of 2.\n",
        "        input_length = torch.LongTensor([orig_frames // 2])\n",
        "\n",
        "        # Squeeze the batch dimension from encoder output (resulting shape: [n_frames, 384])\n",
        "        features = encoder_out.squeeze(0)\n",
        "\n",
        "        return features, input_length, torch.tensor(label_idx, dtype=torch.long)\n",
        "\n",
        "# -----------------------------\n",
        "# Create dataset and split into training and validation sets.\n",
        "# -----------------------------\n",
        "dataset = RespiratoryDataset(label_csv, wav_folder, max_sec=MAX_SEC, whisper_model=whisper_model, class_to_idx=class_to_idx)\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders.\n",
        "batch_size = 8  # You may need to adjust batch size based on available memory.\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Training samples:\", len(train_dataset), \"Validation samples:\", len(val_dataset))\n",
        "\n",
        "# -----------------------------\n",
        "# Define training parameters and loop.\n",
        "# -----------------------------\n",
        "num_epochs = 3\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for features, input_length, labels in data_loader:\n",
        "        # Move data to device.\n",
        "        features = features.to(device)        # shape: [batch, n_frames, feature_dim]\n",
        "        input_length = input_length.to(device)  # shape: [batch]\n",
        "        labels = labels.to(device)              # shape: [batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass through the classification model.\n",
        "        logits = model(features, input_length)  # Expected output: [batch, num_classes]\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = (correct / total) * 100\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for features, input_length, labels in data_loader:\n",
        "            features = features.to(device)\n",
        "            input_length = input_length.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(features, input_length)\n",
        "            loss = criterion(logits, labels)\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = (correct / total) * 100\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# -----------------------------\n",
        "# Training Loop\n",
        "# -----------------------------\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# -----------------------------\n",
        "# Optionally, save the fine-tuned model.\n",
        "# -----------------------------\n",
        "torch.save(model.state_dict(), models_folder_path / \"fine_tuned_rene_model.pt\")\n",
        "print(\"Model saved as fine_tuned_rene_model.pt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
