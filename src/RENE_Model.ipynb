{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7yQy_vx5vaa",
        "outputId": "52397fa7-0651-4297-eed4-84ccfb547fd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-whisper git+https://github.com/sooftware/conformer.git PyYAML gdown gradio -q\n",
        "import torch\n",
        "# Check that we have a GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Create cfg.yaml with model parameters (adapted from the official repo)\n",
        "cat > cfg.yaml << 'CFG'\n",
        "# Data and model config\n",
        "device: 'cuda:0'        # computation device\n",
        "sampling_rate: 8000     # audio sampling rate\n",
        "win_len: 256            # STFT window length (25ms)\n",
        "hop: 80                 # STFT hop length (10ms)\n",
        "lowfreq: 50.0           # mel filterbank low freq cutoff\n",
        "highfreq: 2500.0        # mel filterbank high freq cutoff\n",
        "max_record_time: 16     # max duration of each recording (s)\n",
        "max_event_time: 3       # max duration of each respiratory event (s)\n",
        "# Model hyperparameters\n",
        "whisper_seq: 1500\n",
        "whiper_dim: 384\n",
        "encoder_dim: 256\n",
        "num_encoder_layers: 16\n",
        "num_attention_heads: 4\n",
        "rnn_hid_dim: 512\n",
        "rnn_layers: 2\n",
        "bidirect: true\n",
        "n_fc_layers: 2\n",
        "fc_layer_dim: 1024\n",
        "output_dim: 15\n",
        "input_dropout: 0.1\n",
        "feed_forward_dropout: 0.1\n",
        "attention_dropout: 0.1\n",
        "conv_dropout: 0.1\n",
        "rtb_data_channels: 1\n",
        "CFG\n",
        "\n",
        "%%bash\n",
        "# Create class-id.txt mapping 15 classes (Name|ID)\n",
        "cat > class-id.txt << 'CLASSIDS'\n",
        "Healthy|0\n",
        "Bronchiectasis|1\n",
        "Bronchiolitis|2\n",
        "COPD|3\n",
        "Asthma|4\n",
        "LRTI|5\n",
        "Pneumonia|6\n",
        "URTI|7\n",
        "Bronchitis|8\n",
        "Lung Fibrosis|9\n",
        "Asthma & Lung Fibrosis|10\n",
        "Heart Failure & Lung Fibrosis|11\n",
        "Heart Failure|12\n",
        "Heart Failure & COPD|13\n",
        "Pleural Effusion|14\n",
        "CLASSIDS\n",
        "\n",
        "%%bash\n",
        "# Create cfg_parse.py to load the YAML config\n",
        "cat > cfg_parse.py << 'PYCODE'\n",
        "import yaml\n",
        "cfg = yaml.safe_load(open('cfg.yaml'))\n",
        "PYCODE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3K622F3CMv7",
        "outputId": "518ad14d-691d-4d1c-b3c5-1d86bf750de6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bash: line 31: fg: no job control\n",
            "bash: line 51: fg: no job control\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the config and define model architecture classes\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from cfg_parse import cfg  # load the cfg dictionary from YAML\n",
        "from conformer import Conformer\n",
        "\n",
        "# Depthwise Separable Conv2D layer used in ReneTrialBlock\n",
        "class DSConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        super(DSConv2d, self).__init__()\n",
        "        self.depth_conv = nn.Conv2d(\n",
        "            in_channels=in_channels, out_channels=in_channels,\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            padding=(kernel_size // 2, kernel_size // 2), groups=in_channels\n",
        "        )\n",
        "        self.pointwise_conv = nn.Conv2d(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=(1, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = self.depth_conv(x)\n",
        "        out = self.pointwise_conv(out)\n",
        "        return out\n",
        "\n",
        "# ReneTrialBlock: the final convolutional block that produces class logits\n",
        "class ReneTrialBlock(nn.Module):\n",
        "    def __init__(self, cfg, in_channels):\n",
        "        super(ReneTrialBlock, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        # Left convolution flow\n",
        "        self.left_flow = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(1,1)),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            DSConv2d(in_channels, in_channels, kernel_size=3),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(5,5), padding=(5//2, 5//2))\n",
        "        )\n",
        "        # Right convolution flow (mirror of left_flow with reversed conv order)\n",
        "        self.right_flow = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(5,5), padding=(5//2, 5//2)),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            DSConv2d(in_channels, in_channels, kernel_size=3),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(1,1))\n",
        "        )\n",
        "        # Final linear layer: maps concatenated features to output classes\n",
        "        self.layer = nn.Linear(cfg['rnn_hid_dim'] * 2, cfg['output_dim'])\n",
        "    def forward(self, input_data):\n",
        "        # input_data: [batch, channels*feature_map] as a flat vector\n",
        "        # Reshape to 2D feature maps (assume square)\n",
        "        feature_size = int(math.sqrt(cfg['rnn_hid_dim'] * 2))\n",
        "        x = input_data.reshape(input_data.size(0), cfg['rtb_data_channels'], feature_size, feature_size)\n",
        "        # Convolution flows and residual\n",
        "        out = self.left_flow(x) + self.right_flow(x) + x\n",
        "        # Flatten and linear layer to class logits\n",
        "        out = out.view(input_data.size(0), -1)\n",
        "        return self.layer(out)\n",
        "\n",
        "# Main RENE Model class\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(Model, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        # Conformer encoder (from the installed library)\n",
        "        self.conformer = Conformer(\n",
        "            num_classes=cfg['rnn_hid_dim'],      # output feature dim = rnn hidden dim\n",
        "            input_dim=cfg['whiper_dim'],         # Whisper encoder feature dimension\n",
        "            encoder_dim=cfg['encoder_dim'],\n",
        "            num_encoder_layers=cfg['num_encoder_layers'],\n",
        "            num_attention_heads=cfg['num_attention_heads'],\n",
        "            input_dropout_p=cfg['input_dropout'],\n",
        "            feed_forward_dropout_p=cfg['feed_forward_dropout'],\n",
        "            attention_dropout_p=cfg['attention_dropout'],\n",
        "            conv_dropout_p=cfg['conv_dropout']\n",
        "        )\n",
        "        # Bidirectional GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=cfg['rnn_hid_dim'], hidden_size=cfg['rnn_hid_dim'],\n",
        "            num_layers=cfg['rnn_layers'], bidirectional=cfg['bidirect']\n",
        "        )\n",
        "        # ReneTrialBlock for final classification\n",
        "        self.rene = ReneTrialBlock(cfg, in_channels=cfg['rtb_data_channels'])\n",
        "    def forward(self, x, input_lengths):\n",
        "        # x: [batch, time_frames, whisper_dim], input_lengths: length of each sequence\n",
        "        encoder_out, output_lengths = self.conformer(x, input_lengths)  # [batch, T, rnn_hid_dim]\n",
        "        # Transpose to shape [T, batch, features] for GRU\n",
        "        encoder_out = encoder_out.permute(1, 0, 2)\n",
        "        rnn_out, _ = self.gru(encoder_out)         # rnn_out: [T, batch, 2*rnn_hid_dim] (bi-GRU)\n",
        "        last_timestep = rnn_out[-1]                # take the last time-step output of GRU for each batch\n",
        "        logits = self.rene(last_timestep)    # [batch, output_dim] = class scores\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model (we will load pretrained weights next)\n",
        "model = Model(cfg)\n",
        "print(\"Model instantiated with %d output classes.\" % model.cfg['output_dim'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9T9y4bACN41",
        "outputId": "8b7fbe94-49fd-4364-e7e2-5b9eee315883"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model instantiated with 15 output classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the RENE(S) pretrained checkpoint from Google Drive\n",
        "import os\n",
        "model_path = \"Rene.pth\"\n",
        "if not os.path.exists(model_path):\n",
        "    # Using gdown with the shared file ID\n",
        "    !gdown --id 1NcGPIURY4mWtRr_KkwHAodssOexN-PbC -O Rene.pth\n",
        "else:\n",
        "    print(\"Model checkpoint already downloaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeRMZ89ZCSMS",
        "outputId": "e0b3da4f-26f0-46d2-82d6-77c5487b8fee"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model checkpoint already downloaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained weights into the model\n",
        "checkpoint = torch.load(\"Rene.pth\", map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model_state_dict'], strict=False)  # <--- FIXED HERE\n",
        "model.to(device).eval()\n",
        "print(\"Pretrained RENE model loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyXaV0Y7CUkq",
        "outputId": "7d13163f-10c1-48c5-f21c-51a41402f795"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrained RENE model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a sample lung sound WAV (from SPRSound open dataset)\n",
        "sample_url = \"https://raw.githubusercontent.com/SJTU-YONGFU-RESEARCH-GRP/SPRSound/main/example/65097128_5.6_1_p1_2242.wav\"\n",
        "!wget -q -O sample.wav $sample_url"
      ],
      "metadata": {
        "id": "uaEoapB3CVos"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import whisper\n",
        "\n",
        "# Load Whisper tiny model for feature extraction\n",
        "whisper_model = whisper.load_model(\"tiny\").to(device)\n",
        "whisper_model.eval()\n",
        "\n",
        "# Load and preprocess the audio\n",
        "audio = whisper.load_audio(\"sample.wav\")  # returns NumPy array in float32\n",
        "# Whisper expects 16 kHz audio and pads/clips to 30 sec. We'll pad/trim to 16 sec (target of RENE)\n",
        "MAX_SEC = 30\n",
        "audio = whisper.pad_or_trim(audio, length=MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "\n",
        "# Use Whisper encoder to get audio features\n",
        "with torch.no_grad():\n",
        "    encoder_out = whisper_model.encoder(mel.unsqueeze(0).to(device))  # shape [1, n_frames, 384]\n",
        "# Determine actual length in frames (to inform Conformer)\n",
        "n_frames_total = encoder_out.shape[1]  # typically 1500 for 15s of audio after Whisper padding\n",
        "# Estimate the number of frames corresponding to real (non-padded) audio content\n",
        "orig_len_samples = min(len(audio), MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "orig_frames = math.floor(orig_len_samples / 160)  # 160-sample hop = 10ms frame step\n",
        "input_length = torch.LongTensor([orig_frames // 2])  # //2 because Whisper encoder downsamples by 2x in time\n",
        "\n",
        "# Run the RENE model to get class logits\n",
        "encoder_out = encoder_out.to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(encoder_out, input_length.to(device))\n",
        "    probs = torch.softmax(logits, dim=1)[0]  # probabilities for each of the 15 classes\n",
        "\n",
        "# Load class names and print results\n",
        "classes = [line.split('|')[0] for line in open('class-id.txt').read().splitlines()]\n",
        "top_idx = int(torch.argmax(probs))\n",
        "top_class = classes[top_idx]\n",
        "top_conf = probs[top_idx].item()\n",
        "\n",
        "print(f\"Top predicted class: **{top_class}** ({top_conf*100:.1f}% confidence)\")\n",
        "print(\"\\nClass probabilities:\")\n",
        "ranked = sorted(zip(classes, probs.cpu().numpy()), key=lambda x: x[1], reverse=True)\n",
        "for cls, p in ranked:\n",
        "    print(f\"  {cls:25s}: {p*100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHt0vAUoCXCS",
        "outputId": "10c5016a-7d1b-4a6e-a3c9-dc32f84975c5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top predicted class: **Healthy** (14.9% confidence)\n",
            "\n",
            "Class probabilities:\n",
            "  Healthy                  : 14.90%\n",
            "  URTI                     : 14.63%\n",
            "  Asthma & Lung Fibrosis   : 10.61%\n",
            "  Pneumonia                : 10.19%\n",
            "  Heart Failure            : 6.68%\n",
            "  Heart Failure & Lung Fibrosis: 5.62%\n",
            "  Bronchiolitis            : 5.52%\n",
            "  Pleural Effusion         : 5.16%\n",
            "  Bronchitis               : 5.09%\n",
            "  Lung Fibrosis            : 4.89%\n",
            "  LRTI                     : 3.94%\n",
            "  Asthma                   : 3.70%\n",
            "  Heart Failure & COPD     : 3.38%\n",
            "  COPD                     : 2.95%\n",
            "  Bronchiectasis           : 2.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define the prediction function for Gradio\n",
        "def classify_respiratory_sound(audio_file):\n",
        "    # Load audio from the uploaded file\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    audio = whisper.pad_or_trim(audio, length=MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "    with torch.no_grad():\n",
        "        enc_out = whisper_model.encoder(mel.unsqueeze(0).to(device))\n",
        "    # Calculate original length in frames for masking\n",
        "    orig_len = min(len(audio), MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "    orig_frames = math.floor(orig_len / 160)\n",
        "    inp_len = torch.LongTensor([orig_frames // 2])\n",
        "    with torch.no_grad():\n",
        "        logits = model(enc_out.to(device), inp_len.to(device))\n",
        "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "    # Prepare outputs\n",
        "    top_idx = int(probs.argmax())\n",
        "    top_label = classes[top_idx]\n",
        "    # Build dict of class confidences\n",
        "    confidences = {cls: float(probs[i]) for i, cls in enumerate(classes)}\n",
        "    return top_label, confidences\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=classify_respiratory_sound,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload Lung Sound (.wav)\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Top Predicted Disease\"),\n",
        "        gr.Label(num_top_classes=15, label=\"All Class Probabilities\")\n",
        "    ],\n",
        "    title=\"RENE Respiratory Disease Classifier\",\n",
        "    description=\"Upload a lung sound recording (.wav) to get the predicted respiratory condition and confidence scores for all 15 classes.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app (in Colab, this will display an inline interface or a shareable link)\n",
        "interface.launch(debug=False, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "ag5X2FpKCY6U",
        "outputId": "64529604-265c-4b4a-ba07-2af5f9733e1e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://43ed3c2f803e5ffad8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://43ed3c2f803e5ffad8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing accuracy"
      ],
      "metadata": {
        "id": "0Lqd6K5CFFzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"vbookshelf/respiratory-sound-database\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0S6lSrQFFgk",
        "outputId": "b52398c7-3885-483c-8944-203f2eee3751"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/respiratory-sound-database\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration: Update these paths as needed.\n",
        "# -----------------------------\n",
        "wav_folder = f\"{path}/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/\"               # Folder containing all the .wav files\n",
        "diagnosis_csv = f\"{path}/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv\"  # CSV file with patient diagnoses\n",
        "output_csv = \"/content/labeled_wav_files.csv\"             # Output CSV file\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Load the patient diagnoses CSV.\n",
        "# The CSV provided is in the format:\n",
        "# 101,URTI\n",
        "# 102,Healthy\n",
        "# ...\n",
        "# If the file does not have a header, we specify header=None.\n",
        "# -----------------------------\n",
        "diag_df = pd.read_csv(diagnosis_csv, header=None, names=[\"patient_id\", \"diagnosis\"])\n",
        "\n",
        "# Convert patient IDs to strings (to match with the extracted file name parts)\n",
        "diag_df[\"patient_id\"] = diag_df[\"patient_id\"].astype(str)\n",
        "\n",
        "# Create a mapping from patient_id to diagnosis.\n",
        "diagnosis_map = diag_df.set_index(\"patient_id\")[\"diagnosis\"].to_dict()\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: List all .wav files in the specified directory.\n",
        "# -----------------------------\n",
        "wav_files = [f for f in os.listdir(wav_folder) if f.lower().endswith(\".wav\")]\n",
        "print(f\"Found {len(wav_files)} .wav files.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: For each .wav file, extract the patient ID and look up the diagnosis.\n",
        "# Here we assume the file names start with the patient ID followed by an underscore.\n",
        "# For example: '101_1b1_Al_sc_Meditron.wav' -> Patient ID: '101'\n",
        "# -----------------------------\n",
        "labeled_records = []\n",
        "for file_name in wav_files:\n",
        "    # Extract patient ID by splitting at the underscore.\n",
        "    patient_id = file_name.split(\"_\")[0]\n",
        "    # Retrieve the diagnosis from the mapping.\n",
        "    diagnosis = diagnosis_map.get(patient_id, \"unknown\")\n",
        "    labeled_records.append({\n",
        "        \"wav_file\": file_name,\n",
        "        \"patient_id\": patient_id,\n",
        "        \"diagnosis\": diagnosis\n",
        "    })\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Save the results into an output CSV.\n",
        "# -----------------------------\n",
        "labeled_df = pd.DataFrame(labeled_records)\n",
        "labeled_df.to_csv(output_csv, index=False)\n",
        "print(f\"Labeled file saved to: {output_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxzXL7x1FJLw",
        "outputId": "b2d4deaa-cdef-4538-c777-28cd5ac3bdf7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 920 .wav files.\n",
            "Labeled file saved to: /content/labeled_wav_files.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "wav_folder = f\"{path}/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/\"  # Folder with .wav files\n",
        "label_csv = \"/content/labeled_wav_files.csv\"  # CSV file with columns: wav_file, patient_id, diagnosis\n",
        "class_file = \"/content/class-id.txt\"          # Text file containing class names (each line: \"ClassName| ...\")\n",
        "MAX_SEC = 30  # Maximum duration (in seconds) for padding/trimming\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load Whisper model for feature extraction\n",
        "# -----------------------------\n",
        "whisper_model = whisper.load_model(\"tiny\").to(device)\n",
        "whisper_model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Load your pre-trained RENE classification model.\n",
        "# Make sure to load your model and move it to 'device' and set it to eval mode.\n",
        "# Example (update with your actual code):\n",
        "# model = torch.load(\"path/to/your/rene_model.pt\", map_location=device)\n",
        "# model.eval()\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# -----------------------------\n",
        "# Load class names from class-id.txt.\n",
        "# Each line is split at '|' and the first token is taken as the class name.\n",
        "# -----------------------------\n",
        "with open(class_file, 'r') as f:\n",
        "    classes = [line.split('|')[0].strip() for line in f.read().splitlines()]\n",
        "\n",
        "# -----------------------------\n",
        "# Read the CSV file containing true labels.\n",
        "# Expected CSV format: wav_file,patient_id,diagnosis\n",
        "# -----------------------------\n",
        "df_labels = pd.read_csv(label_csv)\n",
        "print(\"Loaded {} label entries.\".format(len(df_labels)))\n",
        "\n",
        "# -----------------------------\n",
        "# Loop through each file, run through the pipeline, and collect predictions.\n",
        "# For each file, check if the true label is present among the top 3 predictions.\n",
        "# -----------------------------\n",
        "correct = 0\n",
        "total = 0\n",
        "results = []  # To store (file, true_label, predicted_top1, predicted_top3)\n",
        "\n",
        "for idx, row in df_labels.iterrows():\n",
        "    wav_file = row['wav_file']\n",
        "    true_label = row['diagnosis'].strip()  # True diagnosis label (e.g., \"COPD\", \"Healthy\", etc.)\n",
        "    wav_path = os.path.join(wav_folder, wav_file)\n",
        "\n",
        "    # Load and preprocess the audio using Whisper utilities.\n",
        "    audio = whisper.load_audio(wav_path)  # Loads audio as NumPy float32 array\n",
        "    audio = whisper.pad_or_trim(audio, length=MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "\n",
        "    # Get the log-mel spectrogram of the audio.\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "\n",
        "    # Run the Whisper encoder to get features.\n",
        "    with torch.no_grad():\n",
        "        encoder_out = whisper_model.encoder(mel.unsqueeze(0).to(device))  # shape: [1, n_frames, 384]\n",
        "\n",
        "    # Determine the number of frames corresponding to the original audio.\n",
        "    orig_len_samples = min(len(audio), MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "    orig_frames = math.floor(orig_len_samples / 160)  # 160-sample hop (10ms per hop)\n",
        "    input_length = torch.LongTensor([orig_frames // 2])  # Encoder downsamples time by 2Ã—\n",
        "\n",
        "    # Run the RENE classification model.\n",
        "    encoder_out = encoder_out.to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(encoder_out, input_length.to(device))\n",
        "        probs = torch.softmax(logits, dim=1)[0]  # probabilities for each class\n",
        "\n",
        "    # Get the top 3 predicted indices.\n",
        "    topk = torch.topk(probs, k=3)\n",
        "    top_indices = topk.indices.cpu().numpy()\n",
        "    # Top predicted class (highest probability)\n",
        "    top1_label = classes[int(torch.argmax(probs))]\n",
        "    # Top 3 predicted class names\n",
        "    top3_labels = [classes[i] for i in top_indices]\n",
        "\n",
        "    # Count as correct if the true label is among the top 3 predictions.\n",
        "    if true_label.lower() in [label.lower() for label in top3_labels]:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "    results.append((wav_file, true_label, top1_label, top3_labels))\n",
        "    print(f\"[{total}] File: {wav_file} | True: {true_label} | Top1: {top1_label} | Top3: {top3_labels}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Compute and print overall top-3 accuracy.\n",
        "# -----------------------------\n",
        "accuracy = (correct / total) * 100 if total > 0 else 0\n",
        "print(f\"\\nOverall Top-3 Accuracy: {accuracy:.2f}% ({correct} / {total} correct)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "4kbgmbs8F8Rx",
        "outputId": "dc0a7fe8-b66e-4ed9-972d-60c1d38f22e0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 920 label entries.\n",
            "[1] File: 162_1b2_Ar_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n",
            "[2] File: 193_1b2_Pl_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[3] File: 138_2p2_Ll_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Asthma & Lung Fibrosis', 'Healthy']\n",
            "[4] File: 207_2b2_Ar_mc_AKGC417L.wav | True: COPD | Top1: URTI | Top3: ['URTI', 'Healthy', 'Asthma & Lung Fibrosis']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-9626d7835c99>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Load and preprocess the audio using Whisper utilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Loads audio as NumPy float32 array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_or_trim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEC\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/audio.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# fmt: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load audio: {e.stderr.decode()}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                             'failed to raise TimeoutExpired.')\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "wav_folder = f\"{path}/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/\"  # Folder with .wav files\n",
        "label_csv = \"/content/labeled_wav_files.csv\"  # CSV file with columns: wav_file, patient_id, diagnosis\n",
        "class_file = \"/content/class-id.txt\"          # Text file containing class names (each line: \"ClassName| ...\")\n",
        "MAX_SEC = 30  # Maximum duration in seconds for padding/trimming\n",
        "\n",
        "# -----------------------------\n",
        "# Device\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------------\n",
        "# Load Whisper model for feature extraction\n",
        "# -----------------------------\n",
        "whisper_model = whisper.load_model(\"tiny\").to(device)\n",
        "whisper_model.eval()\n",
        "# Freeze Whisper encoder parameters so that they are not updated during fine-tuning\n",
        "for param in whisper_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# -----------------------------\n",
        "# Load your pre-trained RENE classification model.\n",
        "# Replace the following placeholder with your actual model loading code.\n",
        "# For example:\n",
        "# model = torch.load(\"path/to/your/rene_model.pt\", map_location=device)\n",
        "# model.train()  # Set to training mode for fine-tuning.\n",
        "# -----------------------------\n",
        "model.to(device)\n",
        "model.train()  # Set model to training mode\n",
        "\n",
        "# -----------------------------\n",
        "# Load class names from class-id.txt.\n",
        "# Each line in the file is split at the '|' character and the first token is used.\n",
        "# -----------------------------\n",
        "with open(class_file, 'r') as f:\n",
        "    classes = [line.split('|')[0].strip() for line in f.read().splitlines()]\n",
        "print(\"Classes:\", classes)\n",
        "\n",
        "# Create a mapping from class name to index (we assume the CSV 'diagnosis' field exactly matches one of these class names).\n",
        "class_to_idx = {cls: i for i, cls in enumerate(sorted(classes))}\n",
        "num_classes = len(class_to_idx)\n",
        "print(\"Class to index mapping:\", class_to_idx)\n",
        "\n",
        "# -----------------------------\n",
        "# Define a custom dataset.\n",
        "# For each entry, load the .wav file, extract features using Whisper encoder,\n",
        "# and return (encoder_output, input_length, label) for training.\n",
        "# -----------------------------\n",
        "class RespiratoryDataset(Dataset):\n",
        "    def __init__(self, csv_file, wav_folder, max_sec=30, whisper_model=whisper_model, class_to_idx=class_to_idx):\n",
        "        self.df = pd.read_csv(csv_file)\n",
        "        self.wav_folder = wav_folder\n",
        "        self.max_sec = max_sec\n",
        "        self.whisper_model = whisper_model\n",
        "        self.class_to_idx = class_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        wav_file = row['wav_file']\n",
        "        true_label = row['diagnosis'].strip()\n",
        "        label_idx = self.class_to_idx[true_label]\n",
        "        wav_path = os.path.join(self.wav_folder, wav_file)\n",
        "\n",
        "        # Load and preprocess audio\n",
        "        audio = whisper.load_audio(wav_path)  # Returns a NumPy float32 array.\n",
        "        # Pad or trim audio to fixed length\n",
        "        audio = whisper.pad_or_trim(audio, length=self.max_sec * whisper.audio.SAMPLE_RATE)\n",
        "        # Compute log-mel spectrogram\n",
        "        mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "\n",
        "        # Pass through the Whisper encoder (without gradient updates)\n",
        "        with torch.no_grad():\n",
        "            encoder_out = self.whisper_model.encoder(mel.unsqueeze(0).to(device))  # shape: [1, n_frames, 384]\n",
        "\n",
        "        # Determine the number of frames corresponding to the original (non-padded) audio.\n",
        "        orig_len_samples = min(len(audio), self.max_sec * whisper.audio.SAMPLE_RATE)\n",
        "        orig_frames = math.floor(orig_len_samples / 160)  # Whisper uses a hop of 160 samples (~10ms)\n",
        "        # The encoder downsamples time by a factor of 2.\n",
        "        input_length = torch.LongTensor([orig_frames // 2])\n",
        "\n",
        "        # Squeeze the batch dimension from encoder output (resulting shape: [n_frames, 384])\n",
        "        features = encoder_out.squeeze(0)\n",
        "\n",
        "        return features, input_length, torch.tensor(label_idx, dtype=torch.long)\n",
        "\n",
        "# -----------------------------\n",
        "# Create dataset and split into training and validation sets.\n",
        "# -----------------------------\n",
        "dataset = RespiratoryDataset(label_csv, wav_folder, max_sec=MAX_SEC, whisper_model=whisper_model, class_to_idx=class_to_idx)\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders.\n",
        "batch_size = 8  # You may need to adjust batch size based on available memory.\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Training samples:\", len(train_dataset), \"Validation samples:\", len(val_dataset))\n",
        "\n",
        "# -----------------------------\n",
        "# Define training parameters and loop.\n",
        "# -----------------------------\n",
        "num_epochs = 10\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for features, input_length, labels in data_loader:\n",
        "        # Move data to device.\n",
        "        features = features.to(device)        # shape: [batch, n_frames, feature_dim]\n",
        "        input_length = input_length.to(device)  # shape: [batch]\n",
        "        labels = labels.to(device)              # shape: [batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass through the classification model.\n",
        "        logits = model(features, input_length)  # Expected output: [batch, num_classes]\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = (correct / total) * 100\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for features, input_length, labels in data_loader:\n",
        "            features = features.to(device)\n",
        "            input_length = input_length.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(features, input_length)\n",
        "            loss = criterion(logits, labels)\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = (correct / total) * 100\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# -----------------------------\n",
        "# Training Loop\n",
        "# -----------------------------\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# -----------------------------\n",
        "# Optionally, save the fine-tuned model.\n",
        "# -----------------------------\n",
        "torch.save(model.state_dict(), \"fine_tuned_rene_model.pt\")\n",
        "print(\"Model saved as fine_tuned_rene_model.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE5Sigy5KP9Q",
        "outputId": "a673de08-9d75-4f4d-b6bb-3b95807e0183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Classes: ['Healthy', 'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Asthma', 'LRTI', 'Pneumonia', 'URTI', 'Bronchitis', 'Lung Fibrosis', 'Asthma & Lung Fibrosis', 'Heart Failure & Lung Fibrosis', 'Heart Failure', 'Heart Failure & COPD', 'Pleural Effusion']\n",
            "Class to index mapping: {'Asthma': 0, 'Asthma & Lung Fibrosis': 1, 'Bronchiectasis': 2, 'Bronchiolitis': 3, 'Bronchitis': 4, 'COPD': 5, 'Healthy': 6, 'Heart Failure': 7, 'Heart Failure & COPD': 8, 'Heart Failure & Lung Fibrosis': 9, 'LRTI': 10, 'Lung Fibrosis': 11, 'Pleural Effusion': 12, 'Pneumonia': 13, 'URTI': 14}\n",
            "Training samples: 736 Validation samples: 184\n"
          ]
        }
      ]
    }
  ]
}