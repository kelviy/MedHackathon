{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7yQy_vx5vaa",
        "outputId": "90230645-afe0-4c7a-8b05-fab4af1c407d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for conformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install openai-whisper git+https://github.com/sooftware/conformer.git PyYAML gdown gradio -q\n",
        "import torch\n",
        "# Check that we have a GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3K622F3CMv7",
        "outputId": "ac86063e-760b-46b7-8290-64aae6cc6532"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "bash: line 31: fg: no job control\n",
            "bash: line 51: fg: no job control\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Create cfg.yaml with model parameters (adapted from the official repo)\n",
        "cat > cfg.yaml << 'CFG'\n",
        "# Data and model config\n",
        "device: 'cuda:0'        # computation device\n",
        "sampling_rate: 8000     # audio sampling rate\n",
        "win_len: 256            # STFT window length (25ms)\n",
        "hop: 80                 # STFT hop length (10ms)\n",
        "lowfreq: 50.0           # mel filterbank low freq cutoff\n",
        "highfreq: 2500.0        # mel filterbank high freq cutoff\n",
        "max_record_time: 16     # max duration of each recording (s)\n",
        "max_event_time: 3       # max duration of each respiratory event (s)\n",
        "# Model hyperparameters\n",
        "whisper_seq: 1500\n",
        "whiper_dim: 384\n",
        "encoder_dim: 256\n",
        "num_encoder_layers: 16\n",
        "num_attention_heads: 4\n",
        "rnn_hid_dim: 512\n",
        "rnn_layers: 2\n",
        "bidirect: true\n",
        "n_fc_layers: 2\n",
        "fc_layer_dim: 1024\n",
        "output_dim: 15\n",
        "input_dropout: 0.1\n",
        "feed_forward_dropout: 0.1\n",
        "attention_dropout: 0.1\n",
        "conv_dropout: 0.1\n",
        "rtb_data_channels: 1\n",
        "CFG\n",
        "\n",
        "%%bash\n",
        "# Create class-id.txt mapping 15 classes (Name|ID)\n",
        "cat > class-id.txt << 'CLASSIDS'\n",
        "Healthy|0\n",
        "Bronchiectasis|1\n",
        "Bronchiolitis|2\n",
        "COPD|3\n",
        "Asthma|4\n",
        "LRTI|5\n",
        "Pneumonia|6\n",
        "URTI|7\n",
        "Bronchitis|8\n",
        "Lung Fibrosis|9\n",
        "Asthma & Lung Fibrosis|10\n",
        "Heart Failure & Lung Fibrosis|11\n",
        "Heart Failure|12\n",
        "Heart Failure & COPD|13\n",
        "Pleural Effusion|14\n",
        "CLASSIDS\n",
        "\n",
        "%%bash\n",
        "# Create cfg_parse.py to load the YAML config\n",
        "cat > cfg_parse.py << 'PYCODE'\n",
        "import yaml\n",
        "cfg = yaml.safe_load(open('cfg.yaml'))\n",
        "PYCODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9T9y4bACN41",
        "outputId": "750b6071-5202-4704-f7e2-92606af15c0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model instantiated with 15 output classes.\n"
          ]
        }
      ],
      "source": [
        "# Import the config and define model architecture classes\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from cfg_parse import cfg  # load the cfg dictionary from YAML\n",
        "from conformer import Conformer\n",
        "\n",
        "# Depthwise Separable Conv2D layer used in ReneTrialBlock\n",
        "class DSConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "        super(DSConv2d, self).__init__()\n",
        "        self.depth_conv = nn.Conv2d(\n",
        "            in_channels=in_channels, out_channels=in_channels,\n",
        "            kernel_size=(kernel_size, kernel_size),\n",
        "            padding=(kernel_size // 2, kernel_size // 2), groups=in_channels\n",
        "        )\n",
        "        self.pointwise_conv = nn.Conv2d(\n",
        "            in_channels=in_channels, out_channels=out_channels, kernel_size=(1, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = self.depth_conv(x)\n",
        "        out = self.pointwise_conv(out)\n",
        "        return out\n",
        "\n",
        "# ReneTrialBlock: the final convolutional block that produces class logits\n",
        "class ReneTrialBlock(nn.Module):\n",
        "    def __init__(self, cfg, in_channels):\n",
        "        super(ReneTrialBlock, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        # Left convolution flow\n",
        "        self.left_flow = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(1,1)),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            DSConv2d(in_channels, in_channels, kernel_size=3),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(5,5), padding=(5//2, 5//2))\n",
        "        )\n",
        "        # Right convolution flow (mirror of left_flow with reversed conv order)\n",
        "        self.right_flow = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(5,5), padding=(5//2, 5//2)),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            DSConv2d(in_channels, in_channels, kernel_size=3),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=(1,1))\n",
        "        )\n",
        "        # Final linear layer: maps concatenated features to output classes\n",
        "        self.layer = nn.Linear(cfg['rnn_hid_dim'] * 2, cfg['output_dim'])\n",
        "    def forward(self, input_data):\n",
        "        # input_data: [batch, channels*feature_map] as a flat vector\n",
        "        # Reshape to 2D feature maps (assume square)\n",
        "        feature_size = int(math.sqrt(cfg['rnn_hid_dim'] * 2))\n",
        "        x = input_data.reshape(input_data.size(0), cfg['rtb_data_channels'], feature_size, feature_size)\n",
        "        # Convolution flows and residual\n",
        "        out = self.left_flow(x) + self.right_flow(x) + x\n",
        "        # Flatten and linear layer to class logits\n",
        "        out = out.view(input_data.size(0), -1)\n",
        "        return self.layer(out)\n",
        "\n",
        "# Main RENE Model class\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(Model, self).__init__()\n",
        "        self.cfg = cfg\n",
        "        # Conformer encoder (from the installed library)\n",
        "        self.conformer = Conformer(\n",
        "            num_classes=cfg['rnn_hid_dim'],      # output feature dim = rnn hidden dim\n",
        "            input_dim=cfg['whiper_dim'],         # Whisper encoder feature dimension\n",
        "            encoder_dim=cfg['encoder_dim'],\n",
        "            num_encoder_layers=cfg['num_encoder_layers'],\n",
        "            num_attention_heads=cfg['num_attention_heads'],\n",
        "            input_dropout_p=cfg['input_dropout'],\n",
        "            feed_forward_dropout_p=cfg['feed_forward_dropout'],\n",
        "            attention_dropout_p=cfg['attention_dropout'],\n",
        "            conv_dropout_p=cfg['conv_dropout']\n",
        "        )\n",
        "        # Bidirectional GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=cfg['rnn_hid_dim'], hidden_size=cfg['rnn_hid_dim'],\n",
        "            num_layers=cfg['rnn_layers'], bidirectional=cfg['bidirect']\n",
        "        )\n",
        "        # ReneTrialBlock for final classification\n",
        "        self.rene = ReneTrialBlock(cfg, in_channels=cfg['rtb_data_channels'])\n",
        "    def forward(self, x, input_lengths):\n",
        "        # x: [batch, time_frames, whisper_dim], input_lengths: length of each sequence\n",
        "        encoder_out, output_lengths = self.conformer(x, input_lengths)  # [batch, T, rnn_hid_dim]\n",
        "        # Transpose to shape [T, batch, features] for GRU\n",
        "        encoder_out = encoder_out.permute(1, 0, 2)\n",
        "        rnn_out, _ = self.gru(encoder_out)         # rnn_out: [T, batch, 2*rnn_hid_dim] (bi-GRU)\n",
        "        last_timestep = rnn_out[-1]                # take the last time-step output of GRU for each batch\n",
        "        logits = self.rene(last_timestep)    # [batch, output_dim] = class scores\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model (we will load pretrained weights next)\n",
        "model = Model(cfg)\n",
        "print(\"Model instantiated with %d output classes.\" % model.cfg['output_dim'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeRMZ89ZCSMS",
        "outputId": "27502e68-c654-4538-c748-2cf2c194cdf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1NcGPIURY4mWtRr_KkwHAodssOexN-PbC\n",
            "From (redirected): https://drive.google.com/uc?id=1NcGPIURY4mWtRr_KkwHAodssOexN-PbC&confirm=t&uuid=25809376-b14b-4bd7-801d-80c11fd13a9c\n",
            "To: /content/Rene.pth\n",
            "100% 648M/648M [00:07<00:00, 88.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the RENE(S) pretrained checkpoint from Google Drive\n",
        "import os\n",
        "model_path = \"Rene.pth\"\n",
        "if not os.path.exists(model_path):\n",
        "    # Using gdown with the shared file ID\n",
        "    !gdown --id 1NcGPIURY4mWtRr_KkwHAodssOexN-PbC -O Rene.pth\n",
        "else:\n",
        "    print(\"Model checkpoint already downloaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyXaV0Y7CUkq",
        "outputId": "d05a159e-795d-4af9-d8f0-543dadfe95c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained RENE model loaded.\n"
          ]
        }
      ],
      "source": [
        "# Load the pretrained weights into the model\n",
        "checkpoint = torch.load(\"Rene.pth\", map_location='cpu')\n",
        "model.load_state_dict(checkpoint['model_state_dict'], strict=False)  # <--- FIXED HERE\n",
        "model.to(device).eval()\n",
        "print(\"Pretrained RENE model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uaEoapB3CVos"
      },
      "outputs": [],
      "source": [
        "# Download a sample lung sound WAV (from SPRSound open dataset)\n",
        "sample_url = \"https://raw.githubusercontent.com/SJTU-YONGFU-RESEARCH-GRP/SPRSound/main/example/65097128_5.6_1_p1_2242.wav\"\n",
        "!wget -q -O sample.wav $sample_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHt0vAUoCXCS",
        "outputId": "81bb4d29-9a77-4959-9bc4-11d261b6c6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top predicted class: **Healthy** (14.9% confidence)\n",
            "\n",
            "Class probabilities:\n",
            "  Healthy                  : 14.90%\n",
            "  URTI                     : 14.63%\n",
            "  Asthma & Lung Fibrosis   : 10.61%\n",
            "  Pneumonia                : 10.19%\n",
            "  Heart Failure            : 6.68%\n",
            "  Heart Failure & Lung Fibrosis: 5.62%\n",
            "  Bronchiolitis            : 5.52%\n",
            "  Pleural Effusion         : 5.16%\n",
            "  Bronchitis               : 5.09%\n",
            "  Lung Fibrosis            : 4.89%\n",
            "  LRTI                     : 3.94%\n",
            "  Asthma                   : 3.70%\n",
            "  Heart Failure & COPD     : 3.38%\n",
            "  COPD                     : 2.95%\n",
            "  Bronchiectasis           : 2.73%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import whisper\n",
        "\n",
        "# Load Whisper tiny model for feature extraction\n",
        "whisper_model = whisper.load_model(\"tiny\").to(device)\n",
        "whisper_model.eval()\n",
        "\n",
        "# Load and preprocess the audio\n",
        "audio = whisper.load_audio(\"sample.wav\")  # returns NumPy array in float32\n",
        "# Whisper expects 16 kHz audio and pads/clips to 30 sec. We'll pad/trim to 16 sec (target of RENE)\n",
        "MAX_SEC = 30\n",
        "audio = whisper.pad_or_trim(audio, length=MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "\n",
        "# Use Whisper encoder to get audio features\n",
        "with torch.no_grad():\n",
        "    encoder_out = whisper_model.encoder(mel.unsqueeze(0).to(device))  # shape [1, n_frames, 384]\n",
        "# Determine actual length in frames (to inform Conformer)\n",
        "n_frames_total = encoder_out.shape[1]  # typically 1500 for 15s of audio after Whisper padding\n",
        "# Estimate the number of frames corresponding to real (non-padded) audio content\n",
        "orig_len_samples = min(len(audio), MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "orig_frames = math.floor(orig_len_samples / 160)  # 160-sample hop = 10ms frame step\n",
        "input_length = torch.LongTensor([orig_frames // 2])  # //2 because Whisper encoder downsamples by 2x in time\n",
        "\n",
        "# Run the RENE model to get class logits\n",
        "encoder_out = encoder_out.to(device)\n",
        "with torch.no_grad():\n",
        "    logits = model(encoder_out, input_length.to(device))\n",
        "    probs = torch.softmax(logits, dim=1)[0]  # probabilities for each of the 15 classes\n",
        "\n",
        "# Load class names and print results\n",
        "classes = [line.split('|')[0] for line in open('class-id.txt').read().splitlines()]\n",
        "top_idx = int(torch.argmax(probs))\n",
        "top_class = classes[top_idx]\n",
        "top_conf = probs[top_idx].item()\n",
        "\n",
        "print(f\"Top predicted class: **{top_class}** ({top_conf*100:.1f}% confidence)\")\n",
        "print(\"\\nClass probabilities:\")\n",
        "ranked = sorted(zip(classes, probs.cpu().numpy()), key=lambda x: x[1], reverse=True)\n",
        "for cls, p in ranked:\n",
        "    print(f\"  {cls:25s}: {p*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szJUMZlWGp7S"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "id": "ag5X2FpKCY6U",
        "outputId": "ebb2cf0f-0b9f-4572-8771-8ef69a758fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5e6051db410d4ddf9e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://5e6051db410d4ddf9e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define the prediction function for Gradio\n",
        "def classify_respiratory_sound(audio_file):\n",
        "    # Load audio from the uploaded file\n",
        "    audio = whisper.load_audio(audio_file)\n",
        "    audio = whisper.pad_or_trim(audio, length=MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "    with torch.no_grad():\n",
        "        enc_out = whisper_model.encoder(mel.unsqueeze(0).to(device))\n",
        "    # Calculate original length in frames for masking\n",
        "    orig_len = min(len(audio), MAX_SEC * whisper.audio.SAMPLE_RATE)\n",
        "    orig_frames = math.floor(orig_len / 160)\n",
        "    inp_len = torch.LongTensor([orig_frames // 2])\n",
        "    with torch.no_grad():\n",
        "        logits = model(enc_out.to(device), inp_len.to(device))\n",
        "        probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
        "    # Prepare outputs\n",
        "    top_idx = int(probs.argmax())\n",
        "    top_label = classes[top_idx]\n",
        "    # Build dict of class confidences\n",
        "    confidences = {cls: float(probs[i]) for i, cls in enumerate(classes)}\n",
        "    return top_label, confidences\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=classify_respiratory_sound,\n",
        "    inputs=gr.Audio(type=\"filepath\", label=\"Upload Lung Sound (.wav)\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Top Predicted Disease\"),\n",
        "        gr.Label(num_top_classes=15, label=\"All Class Probabilities\")\n",
        "    ],\n",
        "    title=\"RENE Respiratory Disease Classifier\",\n",
        "    description=\"Upload a lung sound recording (.wav) to get the predicted respiratory condition and confidence scores for all 15 classes.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app (in Colab, this will display an inline interface or a shareable link)\n",
        "interface.launch(debug=False, share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
